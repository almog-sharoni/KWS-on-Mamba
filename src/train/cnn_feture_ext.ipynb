{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout):\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch_optimizer import Lookahead\n",
    "\n",
    "from config import dataset, data_loader, model as model_config, optimizer as optimizer_config, scheduler as scheduler_config, training\n",
    "from src.data.data_loader import load_speech_commands_dataset, load_bg_noise_dataset\n",
    "from utils import set_memory_GB,print_model_size, log_to_file\n",
    "from src.utils.augmentations import add_time_shift_and_align, add_silence\n",
    "from train_utils import trainig_loop\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TFDatasetAdapter(Dataset):\n",
    "    def __init__(self, tf_dataset, bg_noise_dataset=None, fixed_length=16000, augmentation=False, noise_level=0.3):\n",
    "        self.tf_dataset = tf_dataset\n",
    "        self.data = list(tf_dataset)\n",
    "        self.bg_noise_data = list(bg_noise_dataset) if bg_noise_dataset is not None else None\n",
    "        self.fixed_length = fixed_length\n",
    "        self.augmentation = augmentation\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio, label = self.data[idx]\n",
    "        audio = audio.numpy()\n",
    "\n",
    "        # Normalize the audio tensor\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "\n",
    "        # Convert to float32\n",
    "        audio = audio.astype(np.float32)\n",
    "\n",
    "        # Ensure the audio tensor has the correct shape (1D array)\n",
    "        if audio.ndim > 1:\n",
    "            audio = np.squeeze(audio)\n",
    "\n",
    "        # Add background noise if available\n",
    "        if self.bg_noise_data:\n",
    "            bg_noise_audio = random.choice(self.bg_noise_data)\n",
    "\n",
    "            # Trim or pad bg_noise to match the audio length\n",
    "            if len(bg_noise_audio) < len(audio):\n",
    "                bg_noise_audio = np.pad(bg_noise_audio, (0, len(audio) - len(bg_noise_audio)), mode='constant')\n",
    "            else:\n",
    "                # Take a random slice of bg_noise_audio with the same length as the original audio\n",
    "                start_idx = random.randint(0, len(bg_noise_audio) - len(audio))\n",
    "                bg_noise_audio = bg_noise_audio[start_idx:start_idx + len(audio)]\n",
    "\n",
    "            # Add bg_noise as noise to the original audio\n",
    "            audio = audio + self.noise_level * bg_noise_audio\n",
    "\n",
    "        # Pad or trim the audio to the fixed length\n",
    "        if len(audio) < self.fixed_length:\n",
    "            audio = np.pad(audio, (0, self.fixed_length - len(audio)), mode='constant')\n",
    "        else:\n",
    "            audio = audio[:self.fixed_length]\n",
    "\n",
    "        # Apply augmentations if any\n",
    "        if self.augmentation:\n",
    "            for aug in self.augmentation:\n",
    "                audio = aug(audio)\n",
    "\n",
    "        return torch.tensor(audio, dtype=torch.float32), torch.tensor(label.numpy(), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append('mamba/mamba_ssm/modules')\n",
    "from mamba_simple import Mamba\n",
    "\n",
    "class KeywordSpottingModel_with_cls(nn.Module):\n",
    "    def __init__(self, input_length, d_model, d_state, d_conv, expand, label_names, num_mamba_layers=1, dropout_rate=0.2):\n",
    "        super(KeywordSpottingModel_with_cls, self).__init__()\n",
    "        \n",
    "        # Initial CNN feature extractor for raw audio input\n",
    "        self.cnn_extractor = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=3, stride=2, padding=1),  # Example: Adjust channels as needed\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, d_model, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # CLS token: learnable parameter with shape [1, 1, d_model]\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        \n",
    "        # Quantization stubs\n",
    "        self.quant = torch.quantization.QuantStub()  # Quantize the input\n",
    "        self.dequant = torch.quantization.DeQuantStub()  # Dequantize output if needed\n",
    "        \n",
    "        # Stack multiple Mamba layers with RMSNorm layer\n",
    "        self.mamba_layers = nn.ModuleList()\n",
    "        self.layer_norms = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_mamba_layers):\n",
    "            self.mamba_layers.append(Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand))\n",
    "            self.layer_norms.append(nn.modules.normalization.RMSNorm(d_model))\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(d_model, len(label_names))  \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to have shape [batch_size, input_length] for raw audio\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        # Add a channel dimension for Conv1D\n",
    "        x = x.unsqueeze(1)  # Shape: [batch_size, 1, input_length]\n",
    "        print(f\"After unsqueeze (for Conv1D): {x.shape}\")\n",
    "\n",
    "        # Pass through the CNN feature extractor\n",
    "        x = self.cnn_extractor(x)  # Shape: [batch_size, d_model, num_frames]\n",
    "        print(f\"After CNN feature extractor: {x.shape}\")\n",
    "\n",
    "        # Transpose to [batch_size, num_frames, d_model] for CLS token addition\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"After permute for CLS token addition: {x.shape}\")\n",
    "        \n",
    "        # Create a CLS token and expand it across the batch dimension\n",
    "        batch_size = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # Shape: [batch_size, 1, d_model]\n",
    "        print(f\"CLS token shape: {cls_tokens.shape}\")\n",
    "        \n",
    "        # Append the CLS token to the input sequence\n",
    "        x = torch.cat((x, cls_tokens), dim=1)  # Shape: [batch_size, num_frames + 1, d_model]\n",
    "        print(f\"After concatenating CLS token: {x.shape}\")\n",
    "        \n",
    "        # Transpose to [batch_size, num_frames + 1, d_model] for Mamba (instead of [batch_size, d_model, num_frames + 1])\n",
    "        x = x.permute(0, 1, 2)\n",
    "        print(f\"After permute for Mamba layer: {x.shape}\")\n",
    "        \n",
    "        # Pass through Mamba layers and layer normalization\n",
    "        for i, (mamba_layer, layer_norm) in enumerate(zip(self.mamba_layers, self.layer_norms)):\n",
    "            x = mamba_layer(x)\n",
    "            print(f\"After Mamba layer {i}: {x.shape}\")\n",
    "            x = layer_norm(x)  # Apply RMSNorm after Mamba layer\n",
    "            print(f\"After RMSNorm {i}: {x.shape}\")\n",
    "\n",
    "        x = self.dropout(x)  # Apply dropout after Mamba layers\n",
    "        print(f\"After dropout: {x.shape}\")\n",
    "        \n",
    "        # Extract the CLS token output (last token)\n",
    "        cls_output = x[:, -1, :]  # Shape: [batch_size, d_model]\n",
    "        print(f\"CLS token output shape: {cls_output.shape}\")\n",
    "        \n",
    "        # Pass through the output layer\n",
    "        x = self.fc(cls_output)\n",
    "        print(f\"Output shape: {x.shape}\")\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory fraction set to 0.044916159152997036\n",
      "Memory fraction in GB: 2.0\n"
     ]
    }
   ],
   "source": [
    "set_memory_GB(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 08:51:26.937652: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-06 08:51:26.959074: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-06 08:51:26.989493: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-06 08:51:27.031535: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-06 08:51:27.041761: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-06 08:51:27.071701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-06 08:51:29.617685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-11-06 08:51:31.028557: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds, silence_ds , info = load_speech_commands_dataset()\n",
    "bg_noise_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4cc819a3b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maintain seed for repructablity\n",
    "np.seed = 42\n",
    "# tf.random.set_seed(42)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes']\n"
     ]
    }
   ],
   "source": [
    "label_names = ['down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes']\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = [\n",
    "    lambda x: add_time_shift_and_align(x),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 08:51:33.977170: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-06 08:51:34.713432: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Convert the TFDS dataset to a PyTorch Dataset for raw audio input\n",
    "fixed_length = 16000  # Length of the raw audio input\n",
    "\n",
    "# Initialize the dataset adapters with raw audio (no MFCC transform)\n",
    "pytorch_train_dataset = TFDatasetAdapter(\n",
    "    train_ds.take(1000),\n",
    "    bg_noise_ds,\n",
    "    fixed_length,\n",
    "    augmentation=augmentations,\n",
    "    noise_level=0.2,\n",
    ")\n",
    "\n",
    "pytorch_val_dataset = TFDatasetAdapter(\n",
    "    val_ds.take(1000),\n",
    "    None,\n",
    "    fixed_length,\n",
    "    augmentation=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #play sound from dataset\n",
    "# import IPython.display as ipd\n",
    "\n",
    "# for i in range(10):\n",
    "#     x, y = pytorch_train_dataset[i]\n",
    "#     print(label_names[y])\n",
    "#     ipd.display(ipd.Audio(x.numpy(), rate=16000))\n",
    "#     # print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader to feed the data into the model\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(pytorch_train_dataset, batch_size=batch_size, shuffle=True,num_workers=4,prefetch_factor=2)\n",
    "val_loader = DataLoader(pytorch_val_dataset, batch_size=batch_size, shuffle=False,num_workers=4,prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With L2 regulariztion AND Droput layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {'d_state': 51, 'd_conv': 10, 'expand': 2, 'batch_size': 26, 'dropout_rate': 0.134439213335519, 'num_mamba_layers': 2, 'n_mfcc': 23, 'n_fft': 475, 'hop_length': 119, 'n_mels': 61, 'noise_level': 0.2582577623788829, 'lr': 0.0011942156978344588, 'weight_decay': 2.5617519345807027e-05}\n",
    "\n",
    "# Configuration parameters from configs dictionary\n",
    "fixed_length = 16000  # Raw audio input length\n",
    "d_model = configs['d_state']  # Output of CNN feature extractor should match d_model for compatibility\n",
    "d_state = configs['d_state']\n",
    "d_conv = configs['d_conv']\n",
    "expand = configs['expand']\n",
    "dropout_rate = configs['dropout_rate']\n",
    "num_mamba_layers = configs['num_mamba_layers']\n",
    "noise_level = configs['noise_level']\n",
    "learning_rate = configs['lr']\n",
    "weight_decay = configs['weight_decay']\n",
    "\n",
    "# Initialize the model with raw audio input length\n",
    "model = KeywordSpottingModel_with_cls(\n",
    "    input_length=fixed_length,  # Use raw audio length instead of MFCC dimensions\n",
    "    d_model=d_model,\n",
    "    d_state=d_state,\n",
    "    d_conv=d_conv,\n",
    "    expand=expand,\n",
    "    label_names=label_names,\n",
    "    num_mamba_layers=num_mamba_layers,\n",
    "    dropout_rate=dropout_rate\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss().to(\"cuda\")  # No need for class weights unless specified\n",
    "\n",
    "# Optimizer setup\n",
    "base_optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer = Lookahead(base_optimizer, k=5, alpha=0.5)  # Wrap around Adam optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                               | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 16000])\n",
      "After unsqueeze (for Conv1D): torch.Size([32, 1, 16000])\n",
      "After CNN feature extractor: torch.Size([32, 51, 2000])\n",
      "After permute for CLS token addition: torch.Size([32, 2000, 51])\n",
      "CLS token shape: torch.Size([32, 1, 51])\n",
      "After concatenating CLS token: torch.Size([32, 2001, 51])\n",
      "After permute for Mamba layer: torch.Size([32, 2001, 51])\n",
      "After Mamba layer 0: torch.Size([32, 2001, 51])\n",
      "After RMSNorm 0: torch.Size([32, 2001, 51])\n",
      "After Mamba layer 1: torch.Size([32, 2001, 51])\n",
      "After RMSNorm 1: torch.Size([32, 2001, 51])\n",
      "After dropout: torch.Size([32, 2001, 51])\n",
      "CLS token output shape: torch.Size([32, 51])\n",
      "Output shape: torch.Size([32, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "  0%|                                                                               | 0/32 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 9\u001b[0m train_accuracies, val_accuracies, train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrainig_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/train_utils.py:39\u001b[0m, in \u001b[0;36mtrainig_loop\u001b[0;34m(model, num_epochs, train_loader, val_loader, criterion, optimizer, scheduler, save_best_model)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:282\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    273\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    274\u001b[0m     (inputs,)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    279\u001b[0m )\n\u001b[1;32m    281\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 282\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:161\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    155\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    160\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 161\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_accuracies, val_accuracies, train_losses, val_losses = trainig_loop(model, num_epochs, train_loader, val_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2592057/1981118585.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for KeywordSpottingModel_with_cls:\n\tMissing key(s) in state_dict: \"total_ops\", \"total_params\", \"quant.total_ops\", \"quant.total_params\", \"dequant.total_ops\", \"dequant.total_params\", \"mamba_layers.total_ops\", \"mamba_layers.total_params\", \"mamba_layers.0.total_ops\", \"mamba_layers.0.total_params\", \"mamba_layers.0.act.total_ops\", \"mamba_layers.0.act.total_params\", \"layer_norms.total_ops\", \"layer_norms.total_params\", \"layer_norms.0.total_ops\", \"layer_norms.0.total_params\". \n\tUnexpected key(s) in state_dict: \"mamba_layers.1.A_log\", \"mamba_layers.1.D\", \"mamba_layers.1.in_proj.weight\", \"mamba_layers.1.conv1d.weight\", \"mamba_layers.1.conv1d.bias\", \"mamba_layers.1.x_proj.weight\", \"mamba_layers.1.dt_proj.weight\", \"mamba_layers.1.dt_proj.bias\", \"mamba_layers.1.out_proj.weight\", \"layer_norms.1.weight\". \n\tsize mismatch for cls_token: copying a param with shape torch.Size([1, 1, 136]) from checkpoint, the shape in current model is torch.Size([1, 1, 52]).\n\tsize mismatch for proj.weight: copying a param with shape torch.Size([136, 69]) from checkpoint, the shape in current model is torch.Size([52, 23]).\n\tsize mismatch for proj.bias: copying a param with shape torch.Size([136]) from checkpoint, the shape in current model is torch.Size([52]).\n\tsize mismatch for mamba_layers.0.A_log: copying a param with shape torch.Size([272, 51]) from checkpoint, the shape in current model is torch.Size([104, 51]).\n\tsize mismatch for mamba_layers.0.D: copying a param with shape torch.Size([272]) from checkpoint, the shape in current model is torch.Size([104]).\n\tsize mismatch for mamba_layers.0.in_proj.weight: copying a param with shape torch.Size([544, 136]) from checkpoint, the shape in current model is torch.Size([208, 52]).\n\tsize mismatch for mamba_layers.0.conv1d.weight: copying a param with shape torch.Size([272, 1, 10]) from checkpoint, the shape in current model is torch.Size([104, 1, 10]).\n\tsize mismatch for mamba_layers.0.conv1d.bias: copying a param with shape torch.Size([272]) from checkpoint, the shape in current model is torch.Size([104]).\n\tsize mismatch for mamba_layers.0.x_proj.weight: copying a param with shape torch.Size([111, 272]) from checkpoint, the shape in current model is torch.Size([106, 104]).\n\tsize mismatch for mamba_layers.0.dt_proj.weight: copying a param with shape torch.Size([272, 9]) from checkpoint, the shape in current model is torch.Size([104, 4]).\n\tsize mismatch for mamba_layers.0.dt_proj.bias: copying a param with shape torch.Size([272]) from checkpoint, the shape in current model is torch.Size([104]).\n\tsize mismatch for mamba_layers.0.out_proj.weight: copying a param with shape torch.Size([136, 272]) from checkpoint, the shape in current model is torch.Size([52, 104]).\n\tsize mismatch for layer_norms.0.weight: copying a param with shape torch.Size([136]) from checkpoint, the shape in current model is torch.Size([52]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 136]) from checkpoint, the shape in current model is torch.Size([10, 52]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#load model\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# load test data\u001b[39;00m\n",
      "\u001b[1;32m      4\u001b[0m pytorch_test_dataset \u001b[38;5;241m=\u001b[39m TFDatasetAdapter(test_ds,\u001b[38;5;28;01mNone\u001b[39;00m, fixed_length, n_mfcc, n_fft, hop_length, n_mels, augmentation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n",
      "\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n",
      "\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n",
      "\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n",
      "\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n",
      "\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n",
      "\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for KeywordSpottingModel_with_cls:\n",
      "\tMissing key(s) in state_dict: \"total_ops\", \"total_params\", \"quant.total_ops\", \"quant.total_params\", \"dequant.total_ops\", \"dequant.total_params\", \"mamba_layers.total_ops\", \"mamba_layers.total_params\", \"mamba_layers.0.total_ops\", \"mamba_layers.0.total_params\", \"mamba_layers.0.act.total_ops\", \"mamba_layers.0.act.total_params\", \"layer_norms.total_ops\", \"layer_norms.total_params\", \"layer_norms.0.total_ops\", \"layer_norms.0.total_params\". \n",
      "\tUnexpected key(s) in state_dict: \"mamba_layers.1.A_log\", \"mamba_layers.1.D\", \"mamba_layers.1.in_proj.weight\", \"mamba_layers.1.conv1d.weight\", \"mamba_layers.1.conv1d.bias\", \"mamba_layers.1.x_proj.weight\", \"mamba_layers.1.dt_proj.weight\", \"mamba_layers.1.dt_proj.bias\", \"mamba_layers.1.out_proj.weight\", \"layer_norms.1.weight\". \n",
      "\tsize mismatch for cls_token: copying a param with shape torch.Size([1, 1, 136]) from checkpoint, the shape in current model is torch.Size([1, 1, 52]).\n",
      "\tsize mismatch for proj.weight: copying a param with shape torch.Size([136, 69]) from checkpoint, the shape in current model is torch.Size([52, 23]).\n",
      "\tsize mismatch for proj.bias: copying a param with shape torch.Size([136]) from checkpoint, the shape in current model is torch.Size([52]).\n",
      "\tsize mismatch for mamba_layers.0.A_log: copying a param with shape torch.Size([272, 51]) from checkpoint, the shape in current model is torch.Size([104, 51]).\n",
      "\tsize mismatch for mamba_layers.0.D: copying a param with shape torch.Size([272]) from checkpoint, the shape in current model is torch.Size([104]).\n",
      "\tsize mismatch for mamba_layers.0.in_proj.weight: copying a param with shape torch.Size([544, 136]) from checkpoint, the shape in current model is torch.Size([208, 52]).\n",
      "\tsize mismatch for mamba_layers.0.conv1d.weight: copying a param with shape torch.Size([272, 1, 10]) from checkpoint, the shape in current model is torch.Size([104, 1, 10]).\n",
      "\tsize mismatch for mamba_layers.0.conv1d.bias: copying a param with shape torch.Size([272]) from checkpoint, the shape in current model is torch.Size([104]).\n",
      "\tsize mismatch for mamba_layers.0.x_proj.weight: copying a param with shape torch.Size([111, 272]) from checkpoint, the shape in current model is torch.Size([106, 104]).\n",
      "\tsize mismatch for mamba_layers.0.dt_proj.weight: copying a param with shape torch.Size([272, 9]) from checkpoint, the shape in current model is torch.Size([104, 4]).\n",
      "\tsize mismatch for mamba_layers.0.dt_proj.bias: copying a param with shape torch.Size([272]) from checkpoint, the shape in current model is torch.Size([104]).\n",
      "\tsize mismatch for mamba_layers.0.out_proj.weight: copying a param with shape torch.Size([136, 272]) from checkpoint, the shape in current model is torch.Size([52, 104]).\n",
      "\tsize mismatch for layer_norms.0.weight: copying a param with shape torch.Size([136]) from checkpoint, the shape in current model is torch.Size([52]).\n",
      "\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 136]) from checkpoint, the shape in current model is torch.Size([10, 52])."
     ]
    }
   ],
   "source": [
    "#load model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "# load test data\n",
    "pytorch_test_dataset = TFDatasetAdapter(test_ds,None, fixed_length, n_mfcc, n_fft, hop_length, n_mels, augmentation=None)\n",
    "test_loader = DataLoader(pytorch_test_dataset, batch_size=batch_size, shuffle=False,num_workers=4,prefetch_factor=2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for audio, labels in test_loader:\n",
    "        audio, labels = audio.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        outputs = model(audio)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        accuracy += (predicted == labels).sum().item()\n",
    "test_accuracy = 100 * accuracy / total\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_learning_curves\n",
    "\n",
    "plot_learning_curves(train_accuracies, val_accuracies, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "\n",
      "MACs: 25464920.0 Which are 0.02546492 Giga-MACs, Params: 19354.0\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n",
      "\u001b[1;32m      5\u001b[0m macs, params \u001b[38;5;241m=\u001b[39m print_model_size(model,input_size\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn(batch_size, input_dim, d_model\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;32m      6\u001b[0m macs \u001b[38;5;241m=\u001b[39m macs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e9\u001b[39m\n",
      "\u001b[0;32m----> 7\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtest_accuracy\u001b[49m\n",
      "\u001b[1;32m      8\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKeywordSpottingModel_RSM_Norm_0-1-2_order_cls_bgnoise\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGMACs\u001b[39m\u001b[38;5;124m'\u001b[39m: [macs], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParams\u001b[39m\u001b[38;5;124m'\u001b[39m: [params], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: [accuracy]}\n",
      "\u001b[1;32m      9\u001b[0m model_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: input_dim, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m'\u001b[39m: d_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_state\u001b[39m\u001b[38;5;124m'\u001b[39m: d_state, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_conv\u001b[39m\u001b[38;5;124m'\u001b[39m: d_conv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m'\u001b[39m: expand}\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import compute_inference_GPU_mem\n",
    "#save model size(macs, params) and accuracy\n",
    "batch_size = configs['batch_size']\n",
    "macs, params = print_model_size(model,input_size=torch.randn(batch_size, input_dim, d_model-1).to(\"cuda\"))\n",
    "macs = macs/1e9\n",
    "accuracy = test_accuracy\n",
    "data = {'Model': ['KeywordSpottingModel_RSM_Norm_0-1-2_order_cls_bgnoise'], 'GMACs': [macs], 'Params': [params], 'Accuracy': [accuracy]}\n",
    "model_config = {'input_dim': input_dim, 'd_model': d_model, 'd_state': d_state, 'd_conv': d_conv, 'expand': expand}\n",
    "data.update(model_config)\n",
    "inf_GPU_mem = compute_inference_GPU_mem(model, input=torch.randn(1, input_dim, d_model-1).to(\"cuda\"))\n",
    "#inference macs and params\n",
    "inf_macs, inf_params = print_model_size(model,input_size=torch.randn(1, input_dim, d_model-1).to(\"cuda\"))\n",
    "inference_data = {'Inference CUDA Mem in MB': [inf_GPU_mem], 'Inference GMACs': [inf_macs/1e9], 'Inference Params': [inf_params]}\n",
    "data.update(inference_data)\n",
    "df = pd.DataFrame(data, index=[0])\n",
    "df.to_csv('results.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.profiler.profile(with_flops=True) as prof:\n",
    "    model(torch.randn(32, input_dim, d_model-1).to(\"cuda\"))\n",
    "\n",
    "# Print FLOPs\n",
    "print(prof.key_averages().table(sort_by=\"flops\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
